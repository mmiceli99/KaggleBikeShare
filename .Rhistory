mutate(count=log(count))
log_bike_recipe <- recipe(count~., data=logTrainData) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_mutate(season=factor(season, levels=1:4, labels=c("Spring", "Summer", "Fall", "Winter"))) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("No", "Yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("No", "Yes"))) %>%
step_time(datetime, features="hour") %>%
step_mutate(hour = factor(hour(datetime), levels=c(0:23), labels=c(0:23))) %>%
step_poly(temp, degree=2) %>%
step_dummy(all_nominal_predictors()) %>% #make dummy variables
step_normalize(all_numeric_predictors())%>% #make mean 0, sd=1
#step_poly(atemp, degree=2) %>%
step_rm(datetime)
prepped_recipe <- prep(log_bike_recipe)
bake(prepped_recipe, new_data = logTrainData) #Make sure recipe work on train
bake(prepped_recipe, new_data = bikeTest)
preg_mod <- linear_reg(penalty=0, mixture = 1) %>%
set_engine("glmnet")
preg_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(preg_mod) %>%
fit(data=logTrainData)
summary(preg_wf)
preg_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(preg_mod) %>%
fit(data=logTrainData)
### PENALIZED REGRESSION MODEL + log scale
library(glmnet)
logTrainData <- bikeTrain %>%
mutate(count=log(count))
log_bike_recipe <- recipe(count~., data=logTrainData) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_mutate(season=factor(season, levels=1:4, labels=c("Spring", "Summer", "Fall", "Winter"))) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("No", "Yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("No", "Yes"))) %>%
step_time(datetime, features="hour") %>%
step_mutate(hour = factor(hour(datetime), levels=c(0:23), labels=c(0:23))) %>%
step_poly(temp, degree=2) %>%
step_dummy(all_nominal_predictors()) %>% #make dummy variables
step_normalize(all_numeric_predictors())%>% #make mean 0, sd=1
#step_poly(atemp, degree=2) %>%
step_rm(datetime)
prepped_recipe <- prep(log_bike_recipe)
bake(prepped_recipe, new_data = logTrainData) #Make sure recipe work on train
bake(prepped_recipe, new_data = bikeTest)
preg_mod <- linear_reg(penalty=tune(), mixture = tune()) %>%
set_engine("glmnet")
preg_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(preg_mod) %>%
fit(data=logTrainData)
tuning_grid <- grid_regular(penalty(),
mixture(),
levels=10)
folds <- vfold_cv(myDataSet, v=10, repeats = 1)
folds <- vfold_cv(logTrainData, v=10, repeats = 1)
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(rmse, mae, rsq))
bestTune <- CV_results%>%
select_best("rmse")
bestTune
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(preg_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(preg_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
test_preds
final_wf
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
library(tidyverse)
library(tidymodels)
library(vroom)
## Read in the data
bikeTrain <- vroom("./train.csv")
bikeTest <- vroom("./test.csv")
## Remove casual and registered because we can't use them to predict
bikeTrain <- bikeTrain %>%
select(-casual, - registered)
### PENALIZED REGRESSION MODEL + log scale
library(glmnet)
logTrainData <- bikeTrain %>%
mutate(count=log(count))
log_bike_recipe <- recipe(count~., data=logTrainData) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_mutate(season=factor(season, levels=1:4, labels=c("Spring", "Summer", "Fall", "Winter"))) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("No", "Yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("No", "Yes"))) %>%
step_time(datetime, features="hour") %>%
step_mutate(hour = factor(hour(datetime), levels=c(0:23), labels=c(0:23))) %>%
step_poly(temp, degree=2) %>%
step_dummy(all_nominal_predictors()) %>% #make dummy variables
step_normalize(all_numeric_predictors())%>% #make mean 0, sd=1
#step_poly(atemp, degree=2) %>%
step_rm(datetime)
prepped_recipe <- prep(log_bike_recipe)
bake(prepped_recipe, new_data = logTrainData) #Make sure recipe work on train
bake(prepped_recipe, new_data = bikeTest)
preg_mod <- linear_reg(penalty=tune(), mixture = tune()) %>%
set_engine("glmnet")
preg_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(preg_mod) %>%
fit(data=logTrainData)
tuning_grid <- grid_regular(penalty(),
mixture(),
levels=10)
folds <- vfold_cv(logTrainData, v=10, repeats = 1)
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(rmse, mae, rsq))
bestTune <- CV_results %>%
select_best("rmse")
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(preg_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write prediction file to CSV
vroom_write(x=test_preds, file="./LogPRegTestPreds.csv", delim=",")
final_wf
collect_metrics(CV_results) %>%
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(rmse))
bestTune <- CV_results %>%
select_best("rmse")
collect_metrics(CV_results) %>%
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(preg_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(preg_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
View(folds)
final_wf
summary(final_wf)
final_wf
summary(final_wf)
describe(final_wf)
summary(final_wf)
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(preg_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
final_wf
bestTune
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
summary(final_wf)
predict(preg_wf, new_data = bikeTest)
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(rmse))
bestTune <- CV_results %>%
select_best("rmse")
bestTune
final_wf <-
preg_wf %>%
finalize_workflow(bestTune)
final_wf
bestTune <- CV_results %>%
select_best("rmse")
bestTune
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
final_wf
preg_mod <- linear_reg(penalty=tune(),
mixture = tune()) %>%
set_engine("glmnet") %>% set_mode('regression')
preg_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(preg_mod) %>%
fit(data=logTrainData)
tuning_grid <- grid_regular(penalty(),
mixture(),
levels=10)
folds <- vfold_cv(logTrainData, v=10, repeats = 1)
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(rmse))
bestTune <- CV_results %>%
select_best("rmse")
bestTune
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
predict(preg_wf, new_data = bikeTest)
bestTune
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(preg_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
final_wf
predict(preg_wf, new_data = bikeTest)
bestTune
final_wf
sessionInfo()
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(final_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write prediction file to CSV
vroom_write(x=test_preds, file="./LogPRegTestPreds.csv", delim=",")
install.packages("rpart")
library(tidyverse)
library(tidymodels)
library(vroom)
library(glmnet)
library(rpart)
## Read in the data
bikeTrain <- vroom("./train.csv")
bikeTest <- vroom("./test.csv")
## Remove casual and registered because we can't use them to predict
bikeTrain <- bikeTrain %>%
select(-casual, - registered)
logTrainData <- bikeTrain %>%
mutate(count=log(count))
log_bike_recipe <- recipe(count~., data=logTrainData) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_mutate(season=factor(season, levels=1:4, labels=c("Spring", "Summer", "Fall", "Winter"))) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("No", "Yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("No", "Yes"))) %>%
step_time(datetime, features="hour") %>%
#Try w/o hour factor first
#step_mutate(hour = factor(hour(datetime), levels=c(0:23), labels=c(0:23))) %>%
#step_poly(temp, degree=2) %>%
#step_dummy(all_nominal_predictors()) %>% #make dummy variables
#step_normalize(all_numeric_predictors())%>% #make mean 0, sd=1
#step_poly(atemp, degree=2) %>%
step_rm(datetime)
prepped_recipe <- prep(log_bike_recipe)
bake(prepped_recipe, new_data = logTrainData) #Make sure recipe work on train
bake(prepped_recipe, new_data = bikeTest)
regT_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(my_mod) %>%
fit(data=logTrainData)
my_mod <- ldecision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use7
set_mode("regression")
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use7
set_mode("regression")
regT_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(my_mod) %>%
fit(data=logTrainData)
regT_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(my_mod) %>%
fit(data=logTrainData)
library(tidyverse)
library(tidymodels)
library(vroom)
library(glmnet)
library(rpart)
## Read in the data
bikeTrain <- vroom("./train.csv")
bikeTest <- vroom("./test.csv")
## Remove casual and registered because we can't use them to predict
bikeTrain <- bikeTrain %>%
select(-casual, - registered)
#put count on log scale
logTrainData <- bikeTrain %>%
mutate(count=log(count))
log_bike_recipe <- recipe(count~., data=logTrainData) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_mutate(season=factor(season, levels=1:4, labels=c("Spring", "Summer", "Fall", "Winter"))) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("No", "Yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("No", "Yes"))) %>%
step_time(datetime, features="hour") %>%
#Try w/o hour factor first
#step_mutate(hour = factor(hour(datetime), levels=c(0:23), labels=c(0:23))) %>%
#step_poly(temp, degree=2) %>%
#step_dummy(all_nominal_predictors()) %>% #make dummy variables
#step_normalize(all_numeric_predictors())%>% #make mean 0, sd=1
#step_poly(atemp, degree=2) %>%
step_rm(datetime)
prepped_recipe <- prep(log_bike_recipe)
bake(prepped_recipe, new_data = logTrainData) #Make sure recipe work on train
bake(prepped_recipe, new_data = bikeTest)
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use7
set_mode("regression")
regT_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(my_mod) %>%
fit(data=logTrainData)
tuning_grid <- grid_regular(tree_depth(),
cost_complexity(),
min_n()
levels=10)
tuning_grid <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels=10)
tuning_grid <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels=10)
folds <- vfold_cv(logTrainData, v=10, repeats = 1)
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(rmse))
CV_results <- regT_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(rmse))
regT_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(my_mod) %>%
fit(data=logTrainData)
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use7
set_mode("regression")
regT_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(my_mod) %>%
fit(data=logTrainData)
regT_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(my_mod) #%>%
tuning_grid <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels=10)
folds <- vfold_cv(logTrainData, v=10, repeats = 1)
CV_results <- regT_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(rmse))
#show what variables to tune
tuning_grid <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels=5)
folds <- vfold_cv(logTrainData, v=5, repeats = 1)
#cross validate and prune your reg tree
CV_results <- regT_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(rmse))
#the best tuning parameters
bestTune <- CV_results %>%
select_best("rmse")
bestTune
#plot various cross validated tuning parameters
collect_metrics(CV_results) %>%
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
final_wf <-
regT_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
summary(final_wf)
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(final_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write prediction file to CSV
vroom_write(x=test_preds, file="./RegTreeTestPreds.csv", delim=",")
log_bike_recipe <- recipe(count~., data=logTrainData) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_mutate(season=factor(season, levels=1:4, labels=c("Spring", "Summer", "Fall", "Winter"))) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("No", "Yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("No", "Yes"))) %>%
step_time(datetime, features="hour") %>%
#Try w/o hour factor first
step_mutate(hour = factor(hour(datetime), levels=c(0:23), labels=c(0:23))) %>%
#step_poly(temp, degree=2) %>%
#step_dummy(all_nominal_predictors()) %>% #make dummy variables
#step_normalize(all_numeric_predictors())%>% #make mean 0, sd=1
#step_poly(atemp, degree=2) %>%
step_rm(datetime)
prepped_recipe <- prep(log_bike_recipe)
bake(prepped_recipe, new_data = logTrainData) #Make sure recipe work on train
bake(prepped_recipe, new_data = bikeTest)
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use7
set_mode("regression")
regT_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(my_mod)
#show what variables to tune
tuning_grid <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels=5)
folds <- vfold_cv(logTrainData, v=5, repeats = 1)
#cross validate and prune your reg tree
CV_results <- regT_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(rmse))
#the best tuning parameters
bestTune <- CV_results %>%
select_best("rmse")
#plot various cross validated tuning parameters
collect_metrics(CV_results) %>%
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
final_wf <-
regT_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
summary(final_wf)
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(final_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write prediction file to CSV
vroom_write(x=test_preds, file="./RegTreeTestPreds.csv", delim=",")
