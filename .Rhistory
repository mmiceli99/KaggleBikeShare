hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='firebrick')
summary(data[,2])
hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='seagreen')
hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='seagreen4')
hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='seagreen4')
hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='seagreen')
summary(data[,2])
hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='seagreen')
summary(data[,2])
hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='seagreen', border ='blue')
var(data[,2])
var(data$B)
hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='seagreen', border ='blue', breaks = 7)
hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='seagreen', border ='black', breaks = 7)
hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='seagreen', border ='black', breaks = 8)
hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='seagreen', border ='black', breaks = 9)
hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='seagreen', border ='black', breaks = 10)
hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='seagreen', border ='black', breaks = 100)
hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='seagreen', border ='black', breaks = 7)
lines(v=mean(data$B))
abline(v=mean(data$B))
abline(v=mean(data$B), col="firebrick")
?abline
abline(v=mean(data$B), col="firebrick", lty=3)
abline(v=mean(data$B), col="firebrick", lty=10)
hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='seagreen', border ='black', breaks = 7)
abline(v=mean(data$B), col="firebrick", lty=10)
hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='seagreen', border ='black', breaks = 7)
abline(v=mean(data$B), col="firebrick", lty=3)
hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='seagreen', border ='black', breaks = 7)
abline(v=mean(data$B), col="firebrick", lty=1)
abline(v=mean(data$B), col="firebrick", lty=1, lwd=5)
hist(data[,2], main="Yeehaw", xlab='Number', ylab = 'B', col='seagreen', border ='black', breaks = 7)
abline(v=mean(data$B), col="firebrick", lty=3, lwd=3)
boxplot(data)
boxplot(data, col='black')
boxplot(data, col='seagreen')
boxplot(data)
?vroom()
library(tidyverse)
?vroom()
?vroom()
library(vroom)
?vroom
data <- read.csv("C:/School/Stat348/KaggleBikeShare/test.csv")
View(data)
vData <- vroom("C:/School/Stat348/KaggleBikeShare/test.csv")
View(vData)
vData <- vData %>%
mutate(as.factor(season))
View(vData)
vData <- vroom("C:/School/Stat348/KaggleBikeShare/test.csv")
View(vData)
vData <- vroom("C:/School/Stat348/KaggleBikeShare/test.csv")
vData$season <- as.factor(vData$season)
vData$holiday <- as.factor(vData$holiday)
vData$workingday <- as.factor(vData$workingday)
vData$weather <- as.factor(vData$weather)
vData <- vroom("C:/School/Stat348/KaggleBikeShare/train.csv")
vData$season <- as.factor(vData$season)
vData$holiday <- as.factor(vData$holiday)
vData$workingday <- as.factor(vData$workingday)
vData$weather <- as.factor(vData$weather)
DataExplorer::plot_intro(vData)
install.packages("DataExplorer")
DataExplorer::plot_intro(vData)
plot_bar <- DataExplorer::plot_bar(vData)
plot_hist <- DataExplorer::plot_histogram(vData)
plot_missing <- DataExplorer::plot_missing(vData)
ggplot(vData) %>%
aes(x=vData$temp, y=vData$windspeed)
ggplot(vData) %>%
aes(x=temp, y=windspeed)
ggplot(vData) %>%
aes(x=temp, y=windspeed) + geom_point()
ggplot(vData, aes(x=temp, y=windspeed)) %>%
geom_point()
ggplot(vData, aes(x=temp, y=windspeed)) +
geom_point()
ggplot(vData, aes(x=temp, y=windspeed)) +
geom_point()
corr(vData)
cor(vData)
ggplot(vData, aes(x=humidity, y=windspeed)) +
geom_point()
ggplot(vData, aes(x=temp, y=atemp)) +
geom_point()
temp_atemp <- ggplot(vData, aes(x=temp, y=atemp)) +
geom_point()
GGally::ggpairs(vData)
install.packages("GGally")
GGally::ggpairs(vData)
plot_intro <- DataExplorer::plot_intro(vData)
plot_bar <- DataExplorer::plot_bar(vData)
plot_hist <- DataExplorer::plot_histogram(vData)
plot_missing <- DataExplorer::plot_missing(vData)
View(vData)
d
date_temp <- ggplot(vData, aes(x=vData$datetime, y=temp)) +
geom_point()
date_temp
date_temp <- ggplot(vData, aes(x=vData$datetime, y=temp)) +
geom_point() +
geom_smooth()
date_temp
date_temp <- ggplot(vData, aes(x=vData$datetime, y=temp)) +
geom_point() +
geom_smooth(se=false)
plot_hist
plot_bar
box_humid <- ggplot(vData, aes(x=humidity))+
geom_boxplot()
box_humid
box_humid <- ggplot(vData, aes(x=humidity, y=season))+
geom_boxplot()
box_humid
weather_bar <- ggplot(vData, aes(x=weather)) +
geom_bar()
weather_bar
library(patchwork)
temp_atemp + date_temp
(temp_atemp + date_temp) / (box_humid + weather_bar)
plot_intro + plot_bar
plot_intro + plot_bar
library(tidyverse)
library(tidymodels)
library(vroom)
## Read in the data
bikeTrain <- vroom("./train.csv")
wd()
## Read in the data
bikeTrain <- vroom("./train.csv")
bikeTest <- vroom("./test.csv")
setwd("C:/School/Stat386")
## Read in the data
bikeTrain <- vroom("./train.csv")
setwd("C:/School/Stat348/KaggleBikeShare")
## Read in the data
bikeTrain <- vroom("./train.csv")
bikeTest <- vroom("./test.csv")
## Remove casual and registered because we can't use them to predict
bikeTrain <- bikeTrain %>%
select(-casual, - registered)
## Cleaning & Feature Engineering
bike_recipe <- recipe(count~., data=bikeTrain) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_mutate(season=factor(season, levels=1:4, labels=c("Spring", "Summer", "Fall", "Winter"))) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("No", "Yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("No", "Yes"))) %>%
step_time(datetime, features="hour") %>%
#step_mutate(hour = factor(hour(datetime), levels=c(0:23), labels=c(0:23))) %>%
step_poly(temp, degree=2) %>%
step_dummy(all_nominal_predictors()) %>% #make dummy variables
step_normalize(all_numeric_predictors())%>% #make mean 0, sd=1
#step_poly(atemp, degree=2) %>%
step_rm(datetime)
## Cleaning & Feature Engineering
bike_recipe <- recipe(count~., data=bikeTrain) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_mutate(season=factor(season, levels=1:4, labels=c("Spring", "Summer", "Fall", "Winter"))) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("No", "Yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("No", "Yes"))) %>%
step_time(datetime, features="hour") %>%
step_mutate(hour = factor(hour(datetime), levels=c(0:23), labels=c(0:23))) %>%
step_poly(temp, degree=2) %>%
step_dummy(all_nominal_predictors()) %>% #make dummy variables
step_normalize(all_numeric_predictors())%>% #make mean 0, sd=1
#step_poly(atemp, degree=2) %>%
step_rm(datetime)
prepped_recipe <- prep(bike_recipe)
bake(prepped_recipe, new_data = bikeTrain) #Make sure recipe work on train
bake(prepped_recipe, new_data = bikeTest) #Make sure recipe works on test
### PENALIZED REGRESSION MODEL + log scale
library(glmnet)
logTrainData <- bikeTrain %>%
mutate(count=log(count))
log_bike_recipe <- recipe(count~., data=logTrainData) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_mutate(season=factor(season, levels=1:4, labels=c("Spring", "Summer", "Fall", "Winter"))) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("No", "Yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("No", "Yes"))) %>%
step_time(datetime, features="hour") %>%
step_poly(temp, degree=2) %>%
step_dummy(all_nominal_predictors()) %>% #make dummy variables
step_normalize(all_numeric_predictors())%>% #make mean 0, sd=1
#step_poly(atemp, degree=2) %>%
step_rm(datetime)
log_bike_recipe <- recipe(count~., data=logTrainData) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_mutate(season=factor(season, levels=1:4, labels=c("Spring", "Summer", "Fall", "Winter"))) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("No", "Yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("No", "Yes"))) %>%
step_time(datetime, features="hour") %>%
step_mutate(hour = factor(hour(datetime), levels=c(0:23), labels=c(0:23))) %>%
step_poly(temp, degree=2) %>%
step_dummy(all_nominal_predictors()) %>% #make dummy variables
step_normalize(all_numeric_predictors())%>% #make mean 0, sd=1
#step_poly(atemp, degree=2) %>%
step_rm(datetime)
prepped_recipe <- prep(log_bike_recipe)
bake(prepped_recipe, new_data = logTrainData) #Make sure recipe work on train
bake(prepped_recipe, new_data = bikeTest)
preg_mod <- linear_reg(penalty=0, mixture = 1) %>%
set_engine("glmnet")
preg_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(preg_mod) %>%
fit(data=logTrainData)
summary(preg_wf)
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(preg_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write prediction file to CSV
vroom_write(x=test_preds, file="./LogPRegTestPreds.csv", delim=",")
library(tidyverse)
library(tidymodels)
library(vroom)
## Read in the data
bikeTrain <- vroom("./train.csv")
bikeTest <- vroom("./test.csv")
## Remove casual and registered because we can't use them to predict
bikeTrain <- bikeTrain %>%
select(-casual, - registered)
logTrainData <- bikeTrain %>%
mutate(count=log(count))
log_bike_recipe <- recipe(count~., data=logTrainData) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_mutate(season=factor(season, levels=1:4, labels=c("Spring", "Summer", "Fall", "Winter"))) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("No", "Yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("No", "Yes"))) %>%
step_time(datetime, features="hour") %>%
step_mutate(hour = factor(hour(datetime), levels=c(0:23), labels=c(0:23))) %>%
step_poly(temp, degree=2) %>%
step_dummy(all_nominal_predictors()) %>% #make dummy variables
step_normalize(all_numeric_predictors())%>% #make mean 0, sd=1
#step_poly(atemp, degree=2) %>%
step_rm(datetime)
prepped_recipe <- prep(log_bike_recipe)
bake(prepped_recipe, new_data = logTrainData) #Make sure recipe work on train
bake(prepped_recipe, new_data = bikeTest)
preg_mod <- linear_reg(penalty=0, mixture = 1) %>%
set_engine("glmnet")
preg_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(preg_mod) %>%
fit(data=logTrainData)
summary(preg_wf)
preg_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(preg_mod) %>%
fit(data=logTrainData)
### PENALIZED REGRESSION MODEL + log scale
library(glmnet)
logTrainData <- bikeTrain %>%
mutate(count=log(count))
log_bike_recipe <- recipe(count~., data=logTrainData) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_mutate(season=factor(season, levels=1:4, labels=c("Spring", "Summer", "Fall", "Winter"))) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("No", "Yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("No", "Yes"))) %>%
step_time(datetime, features="hour") %>%
step_mutate(hour = factor(hour(datetime), levels=c(0:23), labels=c(0:23))) %>%
step_poly(temp, degree=2) %>%
step_dummy(all_nominal_predictors()) %>% #make dummy variables
step_normalize(all_numeric_predictors())%>% #make mean 0, sd=1
#step_poly(atemp, degree=2) %>%
step_rm(datetime)
prepped_recipe <- prep(log_bike_recipe)
bake(prepped_recipe, new_data = logTrainData) #Make sure recipe work on train
bake(prepped_recipe, new_data = bikeTest)
preg_mod <- linear_reg(penalty=tune(), mixture = tune()) %>%
set_engine("glmnet")
preg_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(preg_mod) %>%
fit(data=logTrainData)
tuning_grid <- grid_regular(penalty(),
mixture(),
levels=10)
folds <- vfold_cv(myDataSet, v=10, repeats = 1)
folds <- vfold_cv(logTrainData, v=10, repeats = 1)
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(rmse, mae, rsq))
bestTune <- CV_results%>%
select_best("rmse")
bestTune
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(preg_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(preg_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
test_preds
final_wf
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
library(tidyverse)
library(tidymodels)
library(vroom)
## Read in the data
bikeTrain <- vroom("./train.csv")
bikeTest <- vroom("./test.csv")
## Remove casual and registered because we can't use them to predict
bikeTrain <- bikeTrain %>%
select(-casual, - registered)
### PENALIZED REGRESSION MODEL + log scale
library(glmnet)
logTrainData <- bikeTrain %>%
mutate(count=log(count))
log_bike_recipe <- recipe(count~., data=logTrainData) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_mutate(season=factor(season, levels=1:4, labels=c("Spring", "Summer", "Fall", "Winter"))) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("No", "Yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("No", "Yes"))) %>%
step_time(datetime, features="hour") %>%
step_mutate(hour = factor(hour(datetime), levels=c(0:23), labels=c(0:23))) %>%
step_poly(temp, degree=2) %>%
step_dummy(all_nominal_predictors()) %>% #make dummy variables
step_normalize(all_numeric_predictors())%>% #make mean 0, sd=1
#step_poly(atemp, degree=2) %>%
step_rm(datetime)
prepped_recipe <- prep(log_bike_recipe)
bake(prepped_recipe, new_data = logTrainData) #Make sure recipe work on train
bake(prepped_recipe, new_data = bikeTest)
preg_mod <- linear_reg(penalty=tune(), mixture = tune()) %>%
set_engine("glmnet")
preg_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(preg_mod) %>%
fit(data=logTrainData)
tuning_grid <- grid_regular(penalty(),
mixture(),
levels=10)
folds <- vfold_cv(logTrainData, v=10, repeats = 1)
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(rmse, mae, rsq))
bestTune <- CV_results %>%
select_best("rmse")
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(preg_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write prediction file to CSV
vroom_write(x=test_preds, file="./LogPRegTestPreds.csv", delim=",")
final_wf
collect_metrics(CV_results) %>%
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(rmse))
bestTune <- CV_results %>%
select_best("rmse")
collect_metrics(CV_results) %>%
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(preg_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(preg_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
View(folds)
final_wf
summary(final_wf)
final_wf
summary(final_wf)
describe(final_wf)
summary(final_wf)
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(preg_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
final_wf
bestTune
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
summary(final_wf)
predict(preg_wf, new_data = bikeTest)
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(rmse))
bestTune <- CV_results %>%
select_best("rmse")
bestTune
final_wf <-
preg_wf %>%
finalize_workflow(bestTune)
final_wf
bestTune <- CV_results %>%
select_best("rmse")
bestTune
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
final_wf
preg_mod <- linear_reg(penalty=tune(),
mixture = tune()) %>%
set_engine("glmnet") %>% set_mode('regression')
preg_wf <- workflow()%>%
add_recipe(log_bike_recipe) %>%
add_model(preg_mod) %>%
fit(data=logTrainData)
tuning_grid <- grid_regular(penalty(),
mixture(),
levels=10)
folds <- vfold_cv(logTrainData, v=10, repeats = 1)
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(rmse))
bestTune <- CV_results %>%
select_best("rmse")
bestTune
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
predict(preg_wf, new_data = bikeTest)
bestTune
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=logTrainData)
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(preg_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
final_wf
predict(preg_wf, new_data = bikeTest)
bestTune
final_wf
sessionInfo()
## Get Predictions for test set AND format for Kaggle
test_preds <- predict(final_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred))%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write prediction file to CSV
vroom_write(x=test_preds, file="./LogPRegTestPreds.csv", delim=",")
